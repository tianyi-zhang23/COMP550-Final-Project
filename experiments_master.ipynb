{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "550_experiments_master.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We created a separate notebook for each of our experiments. This is a cleaned version containing all the cells needed to run all of our experiments, which we essentially copy and pasted to different notebooks and changed \"data\", \"size\" and \"p\" parameters for different experiments.\n",
        "\n",
        "Note that although we included code in the github to be able to run experiments via command line, we ended up choosing to use colab to be able to examine the augmented data and see progress (eg. time until dataset augmentation is completed), and only made sure that the colab method of carrying out experiments runs correctly."
      ],
      "metadata": {
        "id": "SBfzc3T3gfrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "Olu9gI6cg5yY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_irQRBagbH9"
      },
      "outputs": [],
      "source": [
        "## Mount to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## requirements\n",
        "## run this cell and restart runtime\n",
        "!pip install -Uqq fastai fastbook\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "!pip install \"pandas>=1.2.0\"\n",
        "!pip install nlpaug\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "pwBLz7XVhKB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/COMP550/Final Project (Git)/src/\n",
        "\n",
        "from utils import *\n",
        "from data import *\n",
        "from fastai.text.all import*\n",
        "\n",
        "import random\n",
        "set_seed(10, True)\n",
        "random.seed(10)"
      ],
      "metadata": {
        "id": "5vfkD-mghP18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual DA Techniques\n"
      ],
      "metadata": {
        "id": "Fj8DWEi3hY1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose dataset size\n",
        "\n",
        "data = 'imdb'   # or 'amz' (amazon), 'agnews', 'yelp', 'yahoo', 'sogou'\n",
        "size = 's'      # or 'm' (medium), 'l' (large)\n",
        "\n",
        "# Choose DA probability\n",
        "p = 0.2"
      ],
      "metadata": {
        "id": "tfPTqb9CiUpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Swap"
      ],
      "metadata": {
        "id": "ILrFMi4ih5JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#augment\n",
        "train_df, test_df = get_dataset(data, size)\n",
        "swapped = train_df.copy()\n",
        "swapped['text'] = swapped['text'].apply(lambda row: random_swap(row, p))\n",
        "train_augmented = pd.concat([train_df, swapped]).drop_duplicates()\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "# train\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "    \n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "#test\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)\n",
        "\n",
        "#save trained model\n",
        "learn.export(fname='/content/drive/MyDrive/COMP550/Final Project/trained_models/imdb_m_swapped_0.1.pkl')"
      ],
      "metadata": {
        "id": "70-WQe0YhhCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Deletion"
      ],
      "metadata": {
        "id": "pl8QRBcFi9PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#augment\n",
        "train_df, test_df = get_dataset(data, size)\n",
        "deleted = train_df.copy()\n",
        "deleted['text'] = deleted['text'].apply(lambda row: random_deletion(row, p))\n",
        "train_augmented = pd.concat([train_df, deleted])\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "#train\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "#test\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "5ttgc1HTi_F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Insertion"
      ],
      "metadata": {
        "id": "1mykJ27IjAUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = get_dataset(data, size)\n",
        "randin = train_df.copy()\n",
        "randin['text'] = randin['text'].apply(lambda row: random_insertion(row, p))\n",
        "train_augmented = pd.concat([train_df, randin])\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "ARgV_Lakjakf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synonym Replacement"
      ],
      "metadata": {
        "id": "bJK_ciCBjna0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = get_dataset(data, size)\n",
        "synrep = train_df.copy()\n",
        "synrep['text'] = synrep['text'].apply(lambda row: synonym_replacement(row, p))\n",
        "train_augmented = pd.concat([train_df, synrep])\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "TPWYdcDejpI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Noise"
      ],
      "metadata": {
        "id": "MQXBwY5IjLm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = get_dataset(data, size)\n",
        "synnoise = train_df.copy()\n",
        "synnoise['text'] = synnoise['text'].apply(lambda row: syntheticnoise(row, p))\n",
        "train_augmented = pd.concat([train_df, synnoise]).drop_duplicates()\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "    \n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "LXt3fJGCk3Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "BaQDGm-GjNFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = get_dataset(data, size)\n",
        "train_copy = train_df.copy()\n",
        "augmented = []\n",
        "for row in train_copy['text']:\n",
        "  choice = random.choice([\"del\", \"ins\", \"swap\", \"syn\"])\n",
        "  if choice == \"del\":\n",
        "    augmented.append(random_deletion(row, p))\n",
        "  elif choice == \"ins\":\n",
        "    augmented.append(random_insertion(row, p))\n",
        "  elif choice == \"swap\":\n",
        "    augmented.append(random_swap(row, p))\n",
        "  else:\n",
        "    augmented.append(synonym_replacement(row, p))\n",
        "train_copy['text'] = augmented \n",
        "train_augmented = pd.concat([train_df, train_copy]).drop_duplicates()\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "XELwkCGjjwUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual Word Embedding"
      ],
      "metadata": {
        "id": "-ZjXb9z3jQQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq fastai fastbook\n",
        "!pip install nltk nlpaug pickle5 transformers\n",
        "!pip install \"pandas>=1.2.0\"\n",
        "from tqdm import tqdm\n",
        "from utils import Language, backtranslation, contextual_word_embeddings\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "kHrUWDdNlfKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation = train_df.copy()\n",
        "augmentation['text'] = augmentation.progress_apply(lambda row: contextual_word_embeddings(row['text']), axis=1)\n",
        "train_lm = TextDataLoaders.from_df(train_df, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_df, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "\n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "\n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "-SfKQuEBlf4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backtranslation"
      ],
      "metadata": {
        "id": "qY1-4-nkjS0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation = train_df.copy()\n",
        "augmentation['text'] = augmentation.progress_apply(lambda row: backtranslation(row['text'], Language.German), axis=1)\n",
        "train_lm = TextDataLoaders.from_df(train_df, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_df, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "\n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "\n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "mSMckIjWlsXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA Combinations"
      ],
      "metadata": {
        "id": "3gWwXyNihcHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose dataset, dataset size and DA probability\n",
        "data = 'agnews'\n",
        "size = 's'\n",
        "\n",
        "p = 0.2"
      ],
      "metadata": {
        "id": "rWjjPMutmrXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combinations that augment an augmented dataset\n",
        "\n",
        "resulting in doubling of training set size. The example we give is adding synthetic noise on top of a dataset augmented by EDA already."
      ],
      "metadata": {
        "id": "IGJ6bW_omcnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = get_dataset(data, size)\n",
        "train_copy = train_df.copy()\n",
        "\n",
        "# first augmentation here. The example given is EDA\n",
        "augmented = []\n",
        "for row in train_copy['text']:\n",
        "  choice = random.choice([\"del\", \"ins\", \"swap\", \"syn\"])\n",
        "  if choice == \"del\":\n",
        "    augmented.append(random_deletion(row, p))\n",
        "  elif choice == \"ins\":\n",
        "    augmented.append(random_insertion(row, p))\n",
        "  elif choice == \"swap\":\n",
        "    augmented.append(random_swap(row, p))\n",
        "  else:\n",
        "    augmented.append(synonym_replacement(row, p))\n",
        "train_copy['text'] = augmented \n",
        "\n",
        "# second augmentation here. The example given is synthetic noise\n",
        "train_copy['text'] = train_copy['text'].apply(lambda row: syntheticnoise(row, p))\n",
        "\n",
        "train_augmented = pd.concat([train_df, train_copy]).drop_duplicates()\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "# train\n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "#test\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "BxgtuBPnmb7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combinations that augment the training set twice using 2 DA techniques\n",
        "and combines all of them together resulting in augmented training set to be triple the original size. The example given is again EDA and Noise."
      ],
      "metadata": {
        "id": "PGzI2H58nQpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = get_dataset(data, size)\n",
        "train_copy = train_df.copy()\n",
        "augmented = []\n",
        "\n",
        "#augmentation set 1 \n",
        "for row in train_copy['text']:\n",
        "  choice = random.choice([\"del\", \"ins\", \"swap\", \"syn\"])\n",
        "  if choice == \"del\":\n",
        "    augmented.append(random_deletion(row, p))\n",
        "  elif choice == \"ins\":\n",
        "    augmented.append(random_insertion(row, p))\n",
        "  elif choice == \"swap\":\n",
        "    augmented.append(random_swap(row, p))\n",
        "  else:\n",
        "    augmented.append(synonym_replacement(row, p))\n",
        "train_copy['text'] = augmented \n",
        "\n",
        "#combining augmented set 1 + original training\n",
        "first_augment = pd.concat([train_df, train_copy]).drop_duplicates()\n",
        "\n",
        "#augmentation set 2 \n",
        "synnoise = train_df.copy()\n",
        "synnoise['text'] = synnoise['text'].apply(lambda row: syntheticnoise(row, p))\n",
        "\n",
        "#combine total augmented ((original + augmentation 1) + augmentation 2)\n",
        "train_augmented = pd.concat([first_augment, synnoise]).drop_duplicates()\n",
        "\n",
        "train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "train_lm.show_batch()\n",
        "\n",
        "#train \n",
        "learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(5, 1e-3)\n",
        "\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "train_class.show_batch()\n",
        "\n",
        "learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "learn = learn.load_encoder('finetuned')\n",
        "    \n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "    \n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "#test\n",
        "test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "acc = learn.validate(dl = test_dl)[1]\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "0W83eOfPl4FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Plots"
      ],
      "metadata": {
        "id": "6IRITO1ghdiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranswap_accs = []\n",
        "randel_accs = []\n",
        "randin_accs = []\n",
        "synrep_accs = []\n",
        "eda_accs = []\n",
        "probs = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "for p in probs:\n",
        "  data = 'imdb'\n",
        "  size = 's'\n",
        "  train_df, test_df = get_dataset(data, size)\n",
        "  swapped = train_df.copy()\n",
        "  swapped['text'] = swapped['text'].apply(lambda row: random_swap(row, p))\n",
        "  train_augmented = pd.concat([train_df, swapped]).drop_duplicates()\n",
        "  train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "  train_lm.show_batch()\n",
        "  learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "  learn.fit_one_cycle(1, 1e-2)\n",
        "  learn.unfreeze()\n",
        "  learn.fit_one_cycle(5, 1e-3)\n",
        "  learn.save_encoder('finetuned')\n",
        "  train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "  train_class.show_batch()\n",
        "  learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "  learn = learn.load_encoder('finetuned')\n",
        "  learn.freeze_to(-2)\n",
        "  learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "  learn.freeze_to(-3)\n",
        "  learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "  test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "  acc = learn.validate(dl = test_dl)[1]\n",
        "  ranswap_accs.append(acc)\n",
        "\n",
        "  train_df, test_df = get_dataset(data, size)\n",
        "  deleted = train_df.copy()\n",
        "  deleted['text'] = deleted['text'].apply(lambda row: random_deletion(row, p))\n",
        "  train_augmented = pd.concat([train_df, deleted]).drop_duplicates()\n",
        "  train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "  train_lm.show_batch()\n",
        "  learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "  learn.fit_one_cycle(1, 1e-2)\n",
        "  learn.unfreeze()\n",
        "  learn.fit_one_cycle(5, 1e-3)\n",
        "  learn.save_encoder('finetuned')\n",
        "  train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "  train_class.show_batch()\n",
        "  learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "  learn = learn.load_encoder('finetuned')\n",
        "  learn.freeze_to(-2)\n",
        "  learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "  learn.freeze_to(-3)\n",
        "  learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "  test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "  acc = learn.validate(dl = test_dl)[1]\n",
        "  randel_accs.append(acc)\n",
        "\n",
        "  train_df, test_df = get_dataset(data, size)\n",
        "  inserted = train_df.copy()\n",
        "  inserted['text'] = inserted['text'].apply(lambda row: random_insertion(row, p))\n",
        "  train_augmented = pd.concat([train_df, inserted]).drop_duplicates()\n",
        "  train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "  train_lm.show_batch()\n",
        "  learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "  learn.fit_one_cycle(1, 1e-2)\n",
        "  learn.unfreeze()\n",
        "  learn.fit_one_cycle(5, 1e-3)\n",
        "  learn.save_encoder('finetuned')\n",
        "  train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "  train_class.show_batch()\n",
        "  learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "  learn = learn.load_encoder('finetuned')\n",
        "  learn.freeze_to(-2)\n",
        "  learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "  learn.freeze_to(-3)\n",
        "  learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "  test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "  acc = learn.validate(dl = test_dl)[1]\n",
        "  randin_accs.append(acc)\n",
        "\n",
        "  train_df, test_df = get_dataset(data, size)\n",
        "  replaced = train_df.copy()\n",
        "  replaced['text'] = replaced['text'].apply(lambda row: synonym_replacement(row, p))\n",
        "  train_augmented = pd.concat([train_df, inserted]).drop_duplicates()\n",
        "  train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "  train_lm.show_batch()\n",
        "  learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "  learn.fit_one_cycle(1, 1e-2)\n",
        "  learn.unfreeze()\n",
        "  learn.fit_one_cycle(5, 1e-3)\n",
        "  learn.save_encoder('finetuned')\n",
        "  train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "  train_class.show_batch()\n",
        "  learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "  learn = learn.load_encoder('finetuned')\n",
        "  learn.freeze_to(-2)\n",
        "  learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "  learn.freeze_to(-3)\n",
        "  learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "  test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "  acc = learn.validate(dl = test_dl)[1]\n",
        "  synrep_accs.append(acc)\n",
        "\n",
        "  train_df, test_df = get_dataset(data, size)\n",
        "  train_copy = train_df.copy()\n",
        "  augmented = []\n",
        "  for row in train_copy['text']:\n",
        "    choice = random.choice([\"del\", \"ins\", \"swap\", \"syn\"])\n",
        "    if choice == \"del\":\n",
        "      augmented.append(random_deletion(row, p))\n",
        "    elif choice == \"ins\":\n",
        "      augmented.append(random_insertion(row, p))\n",
        "    elif choice == \"swap\":\n",
        "      augmented.append(random_swap(row, p))\n",
        "    else:\n",
        "      augmented.append(synonym_replacement(row, p))\n",
        "  train_copy['text'] = augmented \n",
        "  train_augmented = pd.concat([train_df, train_copy]).drop_duplicates()\n",
        "  train_lm = TextDataLoaders.from_df(train_augmented, text_col='text', is_lm=True)\n",
        "  train_lm.show_batch()\n",
        "  learn = language_model_learner(train_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "  learn.fit_one_cycle(1, 1e-2)\n",
        "  learn.unfreeze()\n",
        "  learn.fit_one_cycle(5, 1e-3)\n",
        "  learn.save_encoder('finetuned')\n",
        "  train_class = TextDataLoaders.from_df(train_augmented, text_col='text', label_col='label' ,text_vocab=train_lm.vocab)\n",
        "  train_class.show_batch()\n",
        "  learn = text_classifier_learner(train_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
        "  learn = learn.load_encoder('finetuned')\n",
        "  learn.freeze_to(-2)\n",
        "  learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "  learn.freeze_to(-3)\n",
        "  learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "  test_dl = learn.dls.test_dl(test_df, with_labels=True)\n",
        "  acc = learn.validate(dl = test_dl)[1]\n",
        "  eda_accs.append(acc)"
      ],
      "metadata": {
        "id": "tkvN43xbhnFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(probs, randel_accs, label='Random Deletion')\n",
        "plt.plot(probs,randin_accs, label='Random Insertion')\n",
        "plt.plot(probs,ranswap_accs, label='Random Swap')\n",
        "plt.plot(probs,synrep_accs, label='Synonym Replacement')\n",
        "plt.plot(probs,eda_accs, label='EDA')\n",
        "plt.xlabel('DA probability p')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('IMDB S Accuracies for Individual DA with Various Probabilities')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "4d7i_RLMhqoI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}